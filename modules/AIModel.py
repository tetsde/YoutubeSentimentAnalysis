import pandas as pd
import json
from datetime import datetime
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
import pickle
import os
import glob
from typing import Optional

class SentimentClassifier:
    """
    Sentiment Analysis model using sklearn's MultinomialNB
    Supports 3 classes: Positive (0), Neutral (1), Negative (2)
    """
    
    def __init__(self):
        # sklearn components
        self.vectorizer = CountVectorizer(
            lowercase=True,
            token_pattern=r'\b\w+\b',  # word tokenization
            min_df=1,  # minimum document frequency
        )
        self.model = MultinomialNB(alpha=1.0)  # Laplace smoothing
        
        # Training statistics
        self.is_trained = False
        self.n_samples = 0
        self.class_distribution = {}
    
    def _token(self, text):
        """Simple tokenizer for compatibility"""
        if isinstance(text, str):
            return text.lower().split()
        return []
    
    def _train(self, data):
        """
        Train the model with data from CSV file
        
        Args:
            data (str): Path to training CSV file
        """
        print(f'ƒêang h·ªçc t·ª´ {data}')
        feature = str(input("Nh·∫≠p feature:"))
        label = str(input("Nh·∫≠p label: "))
        
        try:
            # Load data
            df = pd.read_csv(data)
            
            # Convert label to int and filter invalid values
            df['label'] = pd.to_numeric(df['label'], errors='coerce')
            df = df.dropna(subset=['label'])
            df['label'] = df['label'].astype(int)
            
            X = df[feature]
            Y = df[label]
            
            # Vectorize text data
            print("ƒêang vectorize d·ªØ li·ªáu...")
            X_vectorized = self.vectorizer.fit_transform(X)
            
            # Train model
            print("ƒêang train model...")
            self.model.fit(X_vectorized, Y)
            
            # Update statistics
            self.is_trained = True
            self.n_samples = len(Y)
            self.class_distribution = Y.value_counts().to_dict()
            
            print("\nK·∫øt qu·∫£ training:")
            print(f"- T·ªïng s·ªë c√¢u ƒë√£ h·ªçc: {self.n_samples}")
            print(f"- Ph√¢n b·ªë nh√£n:")
            for label_id, count in sorted(self.class_distribution.items()):
                label_name = {0: 'Positive', 1: 'Neutral', 2: 'Negative'}.get(label_id, 'Unknown')
                print(f"  + {label_name} ({label_id}): {count} c√¢u")
            print(f"- Vocabulary size: {len(self.vectorizer.vocabulary_)}")
            
            # Save model
            self._saveModel()
            print("- Xem k·∫øt qu·∫£ chi ti·∫øt t·∫°i file model.pkl")
            
        except FileNotFoundError:
            print("Kh√¥ng t√¨m th·∫•y file")
        except Exception as e:
            print(f"L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh: {e}")
            import traceback
            traceback.print_exc()
    
    def _saveModel(self):
        """Save model using pickle"""
        try:
            model_data = {
                'vectorizer': self.vectorizer,
                'model': self.model,
                'is_trained': self.is_trained,
                'n_samples': self.n_samples,
                'class_distribution': self.class_distribution
            }
            
            with open('modules/model.pkl', 'wb') as f:
                pickle.dump(model_data, f)
            
            print("ƒê√£ l∆∞u model v√†o modules/model.pkl")
        except Exception as e:
            print(f"L·ªói khi l∆∞u model: {e}")
    
    def _loadModel(self):
        """Load model from pickle file"""
        try:
            with open('modules/model.pkl', 'rb') as f:
                model_data = pickle.load(f)
            
            self.vectorizer = model_data['vectorizer']
            self.model = model_data['model']
            self.is_trained = model_data.get('is_trained', True)
            self.n_samples = model_data.get('n_samples', 0)
            self.class_distribution = model_data.get('class_distribution', {})
            
            print("ƒê√£ load model th√†nh c√¥ng!")
            return True
        except FileNotFoundError:
            print("Kh√¥ng t√¨m th·∫•y file model. Vui l√≤ng train model tr∆∞·ªõc.")
            return False
        except Exception as e:
            print(f"L·ªói khi load model: {e}")
            return False
    
    def _find_latest_clean_file(self, data_folder: str = 'data') -> Optional[str]:
        """
        Find the latest clean_comments CSV file in data folder
        
        Args:
            data_folder: Folder containing clean_comments files
            
        Returns:
            Path to latest file or None if not found
        """
        search_pattern = os.path.join(data_folder, 'clean_comments*.csv')
        list_of_files = glob.glob(search_pattern)
        
        if not list_of_files:
            print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y file clean_comments trong folder: {data_folder}")
            return None
        
        latest_file = max(list_of_files, key=os.path.getmtime)
        print(f"üìÅ T√¨m th·∫•y file clean m·ªõi nh·∫•t: {os.path.basename(latest_file)}")
        return latest_file
    
    def predict(self, data: Optional[str] = None, folder: str = "result"):
        """
        Predict sentiment for input data
        Auto-detects latest clean_comments file if data is None
        
        Args:
            data: Path to CSV file, list of texts, or None (auto-detect latest)
            folder: Output folder for results
            
        Returns:
            DataFrame with predictions or None if error
        """
        if not self.is_trained:
            print("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán (is_trained = False).")
            return None
        
        results = []
        
        try:
            # Auto-detect latest clean file if data is None
            if data is None:
                data = self._find_latest_clean_file()
                if data is None:
                    print("‚ùå Kh√¥ng th·ªÉ t√¨m th·∫•y file d·ªØ li·ªáu ƒë·ªÉ d·ª± ƒëo√°n")
                    return None
            
            # Load data
            if isinstance(data, list):
                print(f"--- Nh·∫≠n d·ªØ li·ªáu t·ª´ list ({len(data)} d√≤ng) ---")
                X = data
            else:
                print(f"üìä ƒêang ph√¢n t√≠ch file: {os.path.basename(data)}")
                df = pd.read_csv(data)
                X = df.iloc[:, 0].tolist()  # First column (comment/text)
                print(f"‚úÖ ƒê√£ load {len(X)} d√≤ng d·ªØ li·ªáu")
            
            # Vectorize input
            print(f"B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n {len(X)} d√≤ng...")
            X_vectorized = self.vectorizer.transform(X)
            
            # Predict
            predictions = self.model.predict(X_vectorized)
            
            # Map labels to sentiment names
            label_map = {0: 'Positive', 1: 'Neutral', 2: 'Negative'}
            
            # Create results
            for text, pred_label in zip(X, predictions):
                sentiment = label_map.get(pred_label, 'Unknown')
                results.append({
                    'Text': text,
                    'Label': int(pred_label),
                    'Sentiment': sentiment
                })
            
            # Save results
            result_df = pd.DataFrame(results)
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            output_path = f"{folder}/test_results_{timestamp}.csv"
            
            os.makedirs(folder, exist_ok=True)
            result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f">> Ho√†n t·∫•t! ƒê√£ l∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n t·∫°i: {output_path}")
            
            return result_df
            
        except Exception as e:
            print(f"L·ªói khi d·ª± ƒëo√°n: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def evaluate_fast(self, data_test):
        """
        Evaluate model accuracy on test dataset
        
        Args:
            data_test: Path to test CSV file
        """
        print(f"ƒêang ƒë√°nh gi√° model tr√™n {data_test}...")
        
        try:
            # Predict
            result_df = self.predict(data_test)
            
            if result_df is None:
                print("Kh√¥ng th·ªÉ ƒë√°nh gi√° do qu√° tr√¨nh d·ª± ƒëo√°n g·∫∑p l·ªói.")
                return
            
            # Load true labels
            df_test = pd.read_csv(data_test)
            
            # Convert labels to int
            df_test.iloc[:, 1] = pd.to_numeric(df_test.iloc[:, 1], errors='coerce')
            df_test = df_test.dropna(subset=[df_test.columns[1]])
            df_test.iloc[:, 1] = df_test.iloc[:, 1].astype(int)
            
            y_true = df_test.iloc[:, 1].tolist()
            y_pred = result_df['Label'].tolist()
            
            # Calculate accuracy
            accuracy = accuracy_score(y_true, y_pred)
            print(f"Accuracy: {accuracy * 100:.2f}%")
            
        except Exception as e:
            print(f"L·ªói khi ƒë√°nh gi√°: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    # Initialize model
    model = SentimentClassifier()
    
    # Train model
    model._train("data/train_clean.csv")
    
    # Load model
    model._loadModel()
    
    # Evaluate
    model.evaluate_fast("data/test1.csv")